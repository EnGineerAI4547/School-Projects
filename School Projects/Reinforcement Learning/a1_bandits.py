# -*- coding: utf-8 -*-
"""A1_Bandits.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14-WRtvds9XY4vpynFaGyiVkzxi1jA4zD
"""

# Commented out IPython magic to ensure Python compatibility.

# %matplotlib inline
import numpy as np
from numpy.linalg import inv
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod




from __future__ import division



# Import required libraries
import numpy as np
import matplotlib.pyplot as plt
  
# Define Action class
class Actions:
  def __init__(self, m):
    self.m = m
    self.mean = 0
    self.N = 0
  
  # Choose a random action
  def choose(self): 
    return np.random.randn() + self.m
  
  # Update the action-value estimate
  def update(self, x):
    self.N += 1
    self.mean = (1 - 1.0 / self.N)*self.mean + 1.0 / self.N * x
  
  
def bandit(m1, m2, m3, m4,  m5,  m6,  m7,  m8, m9,  m10,eps, N):
      
  actions = [Actions(m1), Actions(m2), Actions(m3), Actions(m4), Actions(m5), Actions(m6), Actions(m7), Actions(m8), Actions(m9), Actions(m10)]
  
  reward = np.empty(N)
  

    
  for i in range(N):
    # epsilon greedy
    p = np.random.random()
    if p < eps:
      j = np.random.choice(10)
    else:
      j = np.argmax([a.mean for a in actions])
    x = actions[j].choose()
    actions[j].update(x)
  
    # for the plot
    reward[i] = x
  average_reward = np.cumsum(reward) / (np.arange(N) + 1)
  optimal_percentage=100*(len(reward) /(np.arange(N) + 1))
  

  
  for a in actions:
    print(a.mean)
  
  return average_reward,average_reward
  
  
if __name__ == '__main__':
      
  c_1 = bandit(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0,10.0,0.1, 3000)
  plt.figure(figsize = (12, 8))
  plt.plot(c_1[1]*10, label ='eps = 0.1')
  plt.title("Multi-armed bandit")
  plt.ylabel("Percent of Optimal")
  plt.xlabel("Iterations")
  plt.legend()
  plt.show()
  c_0 = bandit(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0,10.0,0, 3000)
  plt.figure(figsize = (12, 8))
  plt.plot(c_0[1]*10, label ='eps = 0.1')
  plt.title("Multi-armed bandit")
  plt.ylabel("Percent of Optimal")
  plt.xlabel("Iterations")
  plt.legend()
  plt.show()
  c_01 = bandit(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0,10.0,0.01, 3000)
  plt.figure(figsize = (12, 8))
  plt.plot(c_01[1]*10, label ='eps = 0.1')
  plt.title("Multi-armed bandit")
  plt.ylabel("Percent of Optimal")
  plt.xlabel("Iterations")
  plt.legend()
  plt.show()


# linear plot
plt.figure(figsize = (12, 8))
plt.plot(c_1[0], label ='eps = 0.1')
plt.plot(c_0[0], label ='eps = 0')
plt.plot(c_01[0], label ='eps = 0.01')
plt.title("Multi-armed bandit")
plt.ylabel("Average Reward")
plt.xlabel("Iterations")
plt.legend()
plt.show()

plt.figure(figsize = (12, 8))
plt.plot(c_1, label ='eps = 0.1')
plt.plot(c_0, label ='eps = 0')
plt.plot(c_01, label ='eps = 0.01')
plt.title("Multi-armed bandit")
plt.ylabel("Percent of Optimal")
plt.xlabel("Iterations")
plt.legend()
plt.show()