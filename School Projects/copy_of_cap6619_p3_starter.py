# -*- coding: utf-8 -*-
"""Copy of CAP6619_P3_STARTER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u8m-K5vYU_zKrg5qrSANYfYFVcPmTbzt

# CAP 6619 - Deep Learning - Dr. Marques - Summer 2022

# Project 3: Machine Learning
## STARTER CODE

### Goals 

- To learn how to implement a Data Science / Machine Learning workflow in Python (using Pandas, Scikit-learn, Matplotlib, and Numpy)
- To learn how to use perform linear regression by least squares using Python and scikit-learn.
- To appreciate that the same linear regression coefficients may be the best fit for dramatically different data distributions -- as illustrated by the Anscombe's quartet.
- To practice with different types of regularization (*lasso* and *ridge*) and understand when to use them.
- To learn how to implement several different machine learning classification models in Python 
- To learn how to evaluate and fine-tune the performance of a model using cross-validation
- To learn how to test a model and produce a set of plots and performance measures
- To expand upon the prior experience of manipulating, summarizing, and visualizing representative datasets  in data science and machine learning

### Instructions

- This assignment is structured in 3 parts, each using their own dataset(s).
- As usual, there will be some Python code to be written and questions to be answered.
- At the end, you should export your notebook to PDF format; it will "automagically" become your report.
- Submit the report (PDF), notebook (.ipynb file), and the link to the "live" version of your solution on Google Colaboratory via Canvas.
- **The number of points is indicated next to each part. They add up to 100.**
- **There are additional (10 points worth of) bonus items**, which are, of course optional.

### Important

- For the sake of reproducibility, use `random_state=0` (or equivalent) in all functions that use random number generation.
- It is OK to attempt the bonus points, but please **do not overdo it!**

---------
### Imports + Google Drive
"""

! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

# Commented out IPython magic to ensure Python compatibility.
# Imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns; sns.set()
import scipy.stats as ss
from pandas_profiling import ProfileReport

from tensorflow import keras
from tensorflow.keras import layers

"""## Part 1: Regression Analysis

-----------
### 1a. Linear regression by least squares

In this part, we will look at the correlation between female literacy and fertility (defined as the average number of children born per woman) throughout the world. For ease of analysis and interpretation, we will work with the *illiteracy* rate.

The Python code below plots the fertility versus illiteracy and computes the Pearson correlation coefficient. The Numpy array `illiteracy` has the illiteracy rate among females for most of the world's nations. The array `fertility` has the corresponding fertility data.
"""

df = pd.read_csv('https://github.com/ogemarques/data-files/raw/main/female_literacy_fertility.csv') 
illiteracy = 100 - df['female literacy']
fertility = df['fertility']

def pearson_r(x, y):
    """Compute Pearson correlation coefficient between two arrays."""
    # Compute correlation matrix: corr_mat
    corr_mat = np.corrcoef(x, y)

    # Return entry [0,1]
    return corr_mat[0,1]

# Plot the illiteracy rate versus fertility
_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')

# Set the margins and label axes
plt.margins(0.02)
_ = plt.xlabel('% illiterate')
_ = plt.ylabel('fertility')

# Show the plot
plt.show()

# Show the Pearson correlation coefficient
print('Pearson correlation coefficient between illiteracy and fertility: {:.5f}'.format(pearson_r(illiteracy, fertility)))

"""### 1.1 Your turn! (5 points)

We will assume that fertility is a linear function of the female illiteracy rate: `f=ai+b`, where `a` is the slope and `b` is the intercept. 

We can think of the intercept as the minimal fertility rate, probably somewhere between one and two. 

The slope tells us how the fertility rate varies with illiteracy. 
We can find the best fit line .

Write code to plot the data and the best fit line (using `np.polyfit()`) and print out the slope and intercept.
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""-------------------
### 1b. Anscombe's quartet

The Anscombe's quartet is a collection of four small data sets that have nearly identical simple descriptive statistics, yet have very different distributions. Each dataset consists of 11 `(x,y)` points. The quartet was created in 1973 by the statistician Francis Anscombe to demonstrate: the importance of visualization and exploratory data analysis (EDA), the effect of outliers and other influential observations on statistical properties, and the limitations of summary statistics (\*).

(\*) See https://heap.io/blog/data-stories/anscombes-quartet-and-why-summary-statistics-dont-tell-the-whole-story if you're interested.

The Python code below performs a linear regression on the data set from Anscombe's quartet that is most reasonably interpreted with linear regression.
"""

x1 = [10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0]
y1 = [8.04, 6.95, 7.58,  8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]

x2 = [10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0]
y2 = [9.14, 8.14, 8.74,  8.77, 9.26, 8.10, 6.13, 3.10, 9.13,  7.26, 4.74]

x3 = [10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0]
y3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15,  6.42, 5.73]

x4 = [8.0,  8.0,  8.0,   8.0,  8.0,  8.0,  8.0,  19.0,  8.0,  8.0,  8.0]
y4 = [6.58, 5.76, 7.71,  8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]

# Perform linear regression: a, b
a, b = np.polyfit(x1, y1, 1)

# Print the slope and intercept
print('slope =', a)
print('intercept =', b)

# Generate theoretical x and y data: x_theor, y_theor
x_theor = np.array([3, 15])
y_theor = a * x_theor + b

# Plot the Anscombe data and theoretical line
_ = plt.plot(x1, y1, marker='.', linestyle='none')
_ = plt.plot(x_theor, y_theor)

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Show the plot
plt.show()

"""### 1.2 Your turn! (5 points)

### Linear regression on all Anscombe data

Write code to verify that all four of the Anscombe data sets have the same slope and intercept from a linear regression, i.e. compute the slope and intercept for each set. 

The data are stored in lists (`anscombe_x = [x1, x2, x3, x4]` and `anscombe_y = [y1, y2, y3, y4]`), corresponding to the $x$ and $y$ values for each Anscombe data set.
"""

anscombe_x = [x1, x2, x3, x4]
anscombe_y = [y1, y2, y3, y4]

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""-------------------
### 1c. Regression using scikit-learn

Now that we know the basics of linear regression, we will switch to scikit-learn, a powerful, workflow-oriented library for data science and machine learning.

The Python code below shows a simple linear regression example using scikit-learn. Note the use of the `fit()` and `predict()` methods.
"""

import matplotlib.pyplot as plt
import numpy as np

# Generate random data around the y = ax+b line where a=3 and b=-2
rng = np.random.RandomState(42)
x = 10 * rng.rand(50)
y = 3 * x - 2 + rng.randn(50)

from sklearn.linear_model import LinearRegression

# Note: If you get a "ModuleNotFoundError: No module named 'sklearn'" error message, don't panic.
# It probably means you'll have to install the module by hand if you're using pip. 
# If you're using conda, you should not see any error message.

model = LinearRegression(fit_intercept=True)

X = x[:, np.newaxis]
X.shape

model.fit(X, y)
print(model.coef_)
print(model.intercept_)

xfit = np.linspace(-1, 11)
Xfit = xfit[:, np.newaxis]
yfit = model.predict(Xfit)

plt.scatter(x, y)
plt.plot(xfit, yfit);

"""### 1d. Polynomial regression

One way to adapt linear regression to nonlinear relationships between variables is to transform the data according to *basis functions*.

The idea is to take the multidimensional linear model:
$$
y = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_3 + \cdots
$$
and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.
That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.

For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:
$$
y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots
$$
Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $a_n$ never multiply or divide each other.
What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$.

The code below shows a simple example of polynomial regression using the ``PolynomialFeatures`` transformer in scikit-learn. Concretely, it shows how we can use polynomial features with a polynomial of degree seven, i.e. $$y = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \cdots + a_7 x^7$$ 

It also introduces the notion of a *pipeline* in scikit-learn. "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters." (https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) 
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
poly_model = make_pipeline(PolynomialFeatures(7),
                           LinearRegression())

rng = np.random.RandomState(1)
x = 10 * rng.rand(100)
y = np.sin(x) + 0.1 * rng.randn(100)

poly_model.fit(x[:, np.newaxis], y)
yfit = poly_model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit);

print('The R^2 score for the fit is: ', poly_model.score(x[:, np.newaxis], y))

"""Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!

### 1.3 Your turn! (10 points)

Write code to find the best degree/order for the polynomial basis functions (between 1 and 15) by computing the quality of the fit using a suitable metric, in this case the $R^2$ coefficient (which can be computer using the `score()` function). 

Remember that **the best possible score is 1.0**. The score can be negative (because the model can be arbitrarily worse). A score of 0 suggests a constant model that always predicts the expected value of y, disregarding the input features.

Hint: If you plot the score against the degree/order of the polynomial, you should see something like this:
![download_curve.png](data:image/png;base64,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)
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### 1e. Regularization

The use of polynomial regression with high-order polynomials can very quickly lead to over-fitting. In this part, we will look into the use of regularization to address potential overfitting.

The code below shows an attempt to fit a 15th degree polynomial to a sinusoidal shaped data. The fit is excellent ($R^2$ > 0.98), but might raise suspicions that it will lead to overfitting.
"""

model = make_pipeline(PolynomialFeatures(15),
                      LinearRegression())
model.fit(x[:, np.newaxis], y)

plt.scatter(x, y)
plt.plot(xfit, model.predict(xfit[:, np.newaxis]))

plt.xlim(0, 10)
plt.ylim(-1.5, 1.5);

score = poly_model.score(x[:, np.newaxis], y)
print(score)

"""### 1.4 Your turn! (5 points)

Write Python code to perform Ridge regression ($L_2$ Regularization), plot the resulting fit, and compute the $R^2$ score.

Hints: 
1. This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator. 
2. In the beginning, use all default values for its parameters.
3. After you get your code to work, spend some time trying to fine-tune the model, i.e., experimenting with the regularization parameters.
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### 1.5 Your turn! (5 points)

Write Python code to perform Lasso regression ($L_1$ Regularization), plot the resulting fit, and compute the $R^2$ score.

Hints: 
1. This type of penalized model is built into Scikit-Learn with the ``Lasso`` estimator. 
2. In the beginning, use `Lasso(alpha=0.1, tol=0.2)`
3. After you get your code to work, spend some time trying to fine-tune the model, i.e., experimenting with the regularization parameters.
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### 1f. The housing problem
The Boston housing dataset is a classic dataset used in linear regression examples.
(See https://scikit-learn.org/stable/datasets/index.html#boston-dataset for more)

The Python code below:
- Loads the Boston dataset (using scikit-learn's `load_boston()`) and converts it into a Pandas dataframe
- Selects two features to be used for fitting a model that will then be used to make predictions: LSTAT (% lower status of the population) and RM (average number of rooms per dwelling) (\*)
- Splits the data into train and test sets

(\*) See https://towardsdatascience.com/linear-regression-on-boston-housing-dataset-f409b7e4a155 for details.
"""

from sklearn.datasets import load_boston
boston_dataset = load_boston()
boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
boston.head()

boston['MEDV'] = boston_dataset.target
X = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns = ['LSTAT','RM'])
y = boston['MEDV']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""#### (OPTIONAL EDA)

The (innocent-looking) lines of code below use Pandas Profiling to produce rich reports, plots and insights on the dataset.

Read more about it:

*   https://pypi.org/project/pandas-profiling/ 
*   https://www.datacourses.com/pandas-1150/ 
*   https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html 
*   https://medium.com/analytics-vidhya/pandas-profiling-5ecd0b977ecd  
"""

# fun with pandas_profiling
profile = ProfileReport(boston, title='Pandas Profiling Report for Boston Housing Dataset', explorative=True)

profile.to_notebook_iframe()

"""### 1.6 Bonus! (10 points)

Write Python code to:

1. Fit a linear model to the data.
2. Compute and print the RMSE and $R^2$ score for both train and test datasets.
3. Fit a polynomial model (of degree 4) to the data.
4. Compute and print the RMSE and $R^2$ score for both train and test datasets.
5. Apply Ridge regression to the polynomial model.
4. Compute and print the RMSE and $R^2$ score for both train and test datasets.
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""-------------------
## Part 2: Classification

### 2a. The Iris dataset 

The Python code below will load a dataset containing information about three types of Iris flowers that had the size of its petals and sepals carefully measured.

The Fisher’s Iris dataset contains 150 observations with 4 features each: 
- sepal length in cm; 
- sepal width in cm; 
- petal length in cm; and 
- petal width in cm. 

The class for each instance is stored in a separate column called “species”. In this case, the first 50 instances belong to class Setosa, the following 50 belong to class Versicolor and the last 50 belong to class Virginica.

See:
https://archive.ics.uci.edu/ml/datasets/Iris for additional information.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
iris = sns.load_dataset("iris")
iris.head()

"""#### Histograms, pair plots and summary statistics 

The code below:

1. Computes and displays relevant summary statistics for the whole dataset.
2. Displays the pair plots for all (4) attributes for all (3) categories / species / classes in the Iris dataset. 
"""

# Display pair plot
sns.pairplot(iris, hue='species', height=2.5);

# Display summary statistics for the whole dataset
iris.describe()

"""### 2.1 Your turn! (15 points)
Write code to: 

1. Build a decision tree classifier using scikit-learn's `DecisionTreeClassifier` (using the default options). Check documentation at https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html 
2. Plot the resulting decision tree. 
(Note: if `graphviz` gives you headaches, a text-based 'plot'-- using `export_text` -- should be OK.)
3. Perform k-fold cross-validation using k=3 and display the results. 

"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### 2b. Digit classification

The MNIST handwritten digit dataset consists of a training set of 60,000 examples, and a test set of 10,000 examples. Each image in the dataset has 28$\times$28 pixels. 

The Python code below loads the images from the MNIST dataset, flattens them, normalizes them (i.e., maps the intensity values from [0..255] to [0..1]), and displays a few images from the training set.
"""

from keras.datasets import mnist

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and validation sets
(X_train, y_train), (X_valid, y_valid) = mnist.load_data()

X_train.shape

y_train.shape

y_train[0:12]

plt.figure(figsize=(5,5))
for k in range(12):
    plt.subplot(3, 4, k+1)
    plt.imshow(X_train[k], cmap='Greys')
    plt.axis('off')
plt.tight_layout()
plt.show()

X_valid.shape

y_valid.shape

y_valid[0]

plt.imshow(X_valid[0], cmap='Greys')
plt.axis('off')
plt.show()

# Reshape (flatten) images 
X_train_reshaped = X_train.reshape(60000, 784).astype('float32')
X_valid_reshaped = X_valid.reshape(10000, 784).astype('float32')

# Scale images to the [0, 1] range
X_train_scaled_reshaped = X_train_reshaped / 255
X_valid_scaled_reshaped = X_valid_reshaped / 255

# Renaming for conciseness
X_training = X_train_scaled_reshaped
X_validation = X_valid_scaled_reshaped

print("X_training shape (after reshaping + scaling):", X_training.shape)
print(X_training.shape[0], "train samples")
print("X_validation shape (after reshaping + scaling):", X_validation.shape)
print(X_validation.shape[0], "validation samples")

# convert class vectors to binary class matrices
y_training = keras.utils.to_categorical(y_train, num_classes)
y_validation = keras.utils.to_categorical(y_valid, num_classes)

print(y_valid[0])
print(y_validation[0])

"""### 2.2 Your turn! (10 points)

Write code to: 

1. Build and fit a 10-class Naive Bayes classifier using scikit-learn's `MultinomialNB()` with default options and using the raw pixel values as features. 
2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix.

Hint: your accuracy will be around 83.5%
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### 2.3 Your turn! (10 points)

Write code to: 

1. Build and fit a 10-class Random Forests classifier using scikit-learn's `RandomForestClassifier()` with default options (don't forget `random_state=0`) and using the raw pixel values as features. 
2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. 

Hint: your accuracy should be > 90%
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### 2.4 Your turn! (10 points)

Write code to: 

1. Build and fit a 10-class classifier of your choice, with sensible initialization options, and using the raw pixel values as features. 
2. Make predictions on the test data, compute the overall accuracy and plot the resulting confusing matrix. 

Hint: A variation of the Random Forests classifier from 2.2 above is acceptable. In that case, document your selection of (hyper)parameters and your rationale for choosing them.
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""## Part 3: Face Recognition using PCA (eigenfaces)

In this part you will build a face recognition solution.

We will use a subset of the Labeled Faces in the Wild (LFW) people dataset: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html

The Python code below loads a dataset of 1867 images (resized to 62 $\times$ 47 pixels) from the dataset and displays some of them.
"""

from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=40)
print(faces.target_names)
print(faces.images.shape)

plt.rcParams["figure.figsize"]=15,15
fig, ax = plt.subplots(3, 5)
for i, axi in enumerate(ax.flat):
    axi.imshow(faces.images[i], cmap='bone')
    axi.set(xticks=[], yticks=[],
            xlabel=faces.target_names[faces.target[i]])

"""#### 3.1 Your turn! (15 points)

Write code to: 

1. Use Principal Component Analysis (PCA) to reduce the dimensionality of each face to the first 120 components. 
2. Build and fit a multi-class SVM classifier, with sensible initialization options, and using the PCA-reduced  features. 
3. Make predictions on the test data, compute the precision, recall and f1 score for each category, compute the overall accuracy, and plot the resulting confusing matrix. 
4. Display examples of correct and incorrect predictions (at least 5 of each). 
"""

# ENTER YOUR CODE HERE
# ...
# ...
# ...

"""### Conclusions (10 points)

Write your conclusions and make sure to address the issues below:
- What have you learned from this assignment?
- Which parts were the most fun, time-consuming, enlightening, tedious?
- What would you do if you had an additional week to work on this?

*Enter your conclusions here*
"""