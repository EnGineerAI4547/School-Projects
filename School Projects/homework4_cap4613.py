# -*- coding: utf-8 -*-
"""Homework4_CAP4613.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DaRkMnD-1uGGaFTMgcL4paATvj2WObol
"""

#Call the required libraries 
import numpy as np 
import matplotlib.pyplot as plt 

# Class to create a neural network with single neuron 
class NeuralNetwork(object):

  def __init__(self, learning_r):
 # Using seed to make sure it'll generate same weights in every run 
    np.random.seed(1) 
 # 3x1 Weight matrix 
    self.weight_matrix =np.array([[1],[1],[1]], dtype=np.float) #  # hard_limiter as activation fucntion 
 # hard_limiter as activation fucntion 
    self.l_rate=learning_r 
    self.history={'train_acc': np.zeros (0), 
    'train_cost': np.zeros (0), 
    'weight_matrix': np.zeros ((3,0))}  
  def sigmoid(self, x): 
    return x 
 # forward propagation 
  def forward_propagation(self, inputs): 
    outs=np.dot(inputs, self.weight_matrix) 
    return self.sigmoid(outs) 
 #forward propagation 
  def pred(self, inputs): 
    prob=self.forward_propagation(inputs) 
    preds=np.ones(prob.shape) 
    preds[prob<0]=0 #for linear, and >0.5 for sigmoid fnction  return preds 
 #forward propagation 
  def train_acc_fun(self, inputs, labels): 
    preds=self.pred(inputs) 
    acc=np.sum(labels==preds)/len(labels)*100 
    return acc 
#training the neural network using gradient descent learning.  
  def train_GDL(self,train_inputs, train_outputs, num_train_epochs=1000): 
    N=train_inputs.shape[0] 
 #Number of Epoch we want to perform for this set of input.  
    for epoch in range(num_train_epochs): 
      print('Epoch#' +str(epoch)) 
      outputs=self.forward_propagation(train_inputs)
 #Calculate the error in the output. 
      error=train_outputs-outputs 
      adjustment=(self.l_rate/N)*np.sum(np.multiply(error,train_inputs),axis=0) 
 #Adjust the weight matrix 
      self.weight_matrix[:,0] +=adjustment 
 #Find the cost and the accuracy of the training  
      train_acc=self.train_acc_fun(train_inputs,train_outputs)  
      train_cost=(1/(2*N))*np.sum(error**2) 
      self.history['train_acc']=np.append(self.history['train_acc'],train_acc) 
      self.history['train_cost']=np.append(self.history['train_cost'],train_cost) 
      self.history['weight_matrix']=np.append(self.history['weight_matrix'], self.weight_matrix, axis=1) 
def plot_fun(features,labels,classes): 
 #ploting the data points 
  plt.plot(features[labels[:]==classes[0],0], features[labels[:]==classes[ 0],1], 'rs', features[labels[:]==classes[1],0], features[labels[:]==classes[1] ,1], 'g^') 
  plt.axis([-2,2,-2,2]) 
  plt.xlabel('x: feature 1') 
  plt.ylabel('y: feature 2') 
  plt.legend(['Class'+str(classes[0]), 'Class'+str(classes[1])])  
  plt.show() 
def plot_fun_thr(features,labels,thre_parms,classes): 
 #ploting the data points 
  plt.plot(features[labels[:]==classes[0],0], features[labels[:]==classes[ 0],1], 'rs', features[labels[:]==classes[1],0], features[labels[:]==classes[1] ,1], 'g^') 
 #ploting the seperating line 
  x1 = np.linspace(-2,2,50) 
  x2 = -(thre_parms[0]*x1+thre_parms[2])/thre_parms[1] #a X1 + b X2 + c=0 -- > x2 = -(a X1 + c)/b 
  plt.plot(x1, x2, '-r') 
  plt.xlabel('x: feature 1') 
  plt.ylabel('y: feature 2') 
  plt.legend(['Class'+str(classes[0]), 'Class'+str(classes[1])])  
  plt.show()
def plot_curve(accuracy_train,cost_train, title, step=1):
  epochs=np.arange(0,cost_train.shape[0],step) 
  plt.subplot(1,2,1) 
  plt.plot(epochs,accuracy_train[::step]) 
  plt.axis([-1,2,-1,2]) 
  plt.xlabel('Epoch#') 
  plt.ylabel('Accuracy') 
  plt.title('Training Accuracy') 
  plt.subplot(1,2,2) 
  plt.plot(epochs,cost_train[::step]) # you were using accuracy_train inst ead 
 #plt.axis([-1,2,-1,2]) 
  plt.xlabel('Epoch#') 
  plt.ylabel('Cost(MSE)') 
  plt.title('Training Cost') 
  plt.suptitle(title) 
  plt.show() 


#main function
features=np.array([[1,1], [1,0], [0,1], [-1,-1], [0.5,3], [0.7,2], [- 1,0], [-1,1],[2,0], [-2,-1]]) 
labels=np.array([1,1,0,0,1,1,0,0,1,0]) 
classes=[0,1] 
plot_fun(features,labels,classes)

#expanding the feature space by adding the bias vector 
bias=np.ones((features.shape[0],1)) 
features=np.append(features,bias,axis=1)
for learning_r_i in [1,0.5,0.05]: 
  neural_network=NeuralNetwork(learning_r=learning_r_i) 
  neural_network.train_GDL(features,np.expand_dims(labels,axis=1),num_train_epochs=50) 
 #plot the learning curves with step of 5 epochs 
  plot_curve(neural_network.history['train_acc'],neural_network.history['train_cost'], title='Learning curves - Learning rate=' +str(learning_r_i),step=1 ) 
 #plot the seperating line based on the weights 
  plt.figure() 
  for epoch in np.arange (0,50,5): 
    plot_fun_thr(features[:,0:2],labels,neural_network.history['weight_matrix'][:,epoch],classes)
   # train_inputs without bias 
    plt.show()

